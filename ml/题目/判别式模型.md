
## [知乎问题](https://www.zhihu.com/question/20446337)

![](https://pic2.zhimg.com/80/v2-a2e753542fc6384ee351cabdbe6dd523_hd.jpg)

机器学习的任务$T$是从属性X预测标记Y，即求概率$P(Y|X)$；

对于判别式模型来说求得$P(Y|X)$，对未见示例X，根据P(Y|X)可以求得标记Y，即可以直接判别出来，如上图的左边所示，实际是就是直接得到了判别边界，所以传统的、耳熟能详的机器学习算法如线性回归模型、支持向量机SVM等都是判别式模型，这些模型的**特点**都是输入属性X可以直接得到Y（对于二分类任务来说，实际得到一个score，当score大于threshold时则为正类，否则为反类）（根本原因个人认为是对于某示例X_1，对正例和反例的标记的条件概率之和等于1，即$P(Y_1|X_1)+P(Y_2|X_1)=1$

而生成式模型求得P(Y,X)，对于未见示例X，你要求出X与不同标记之间的联合概率分布，然后大的获胜，如上图右边所示，并没有什么边界存在，对于未见示例（红三角），求两个联合概率分布（有两个类），比较一下，取那个大的。机器学习中朴素贝叶斯模型、隐马尔可夫模型HMM等都是生成式模型，熟悉Naive Bayes的都知道，对于输入X，需要求出好几个联合概率，然后较大的那个就是预测结果（根本原因个人认为是对于某示例X_1，对正例和反例的标记的联合概率不等于1，即P(Y_1,X_1)+P(Y_2,X_1)<1，要遍历所有的X和Y的联合概率求和，即sum(P(X,Y))=1，具体可参见楼上woodyhui提到的维基百科Generative model里的例子）

换个角度，判别式模型（Discriminative Model）是直接对条件概率p(y|x;θ)建模。
　　举例：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。
  
生成式模型（Generative Model）则会对x和y的联合分布p(x,y)建模，然后通过贝叶斯公式来求得p(yi|x)，然后选取使得p(yi|x)最大的yi，

$$\underset{y}{\operatorname{arg \ max}} \ \ p(y|x) \\  
=\underset{y}{\operatorname{arg \ max}} \ \ \frac{p(x|y)p(y)}{p(x)} \\
=\underset{y}{\operatorname{arg \ max}} \ \ p(x|y)p(y)
$$

　　举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。

关于二者之间的优劣有大量的讨论. SVM的发明者Vapnik声称"one should solve the (classification) problem directly and never solve a more general problem as an intermediate step [such as modeling p(x|y)]", 但是, 最近Deep Learning大行其道, 其代表性算法DBN就是生成式模型. 通常来说, 因为生成式模型要对类条件密度(class conditional density)\(p(x|y_i)\)进行建模, 而判别式模型只需要对类后验密度(class-posterior density)进行建模, 前者通常会比后者要复杂, 更难以建模

## 《统计学习方法》1.7节

监督学习的任务就是学习一个模型，应用这一模型，对给定的输入预测相应的输出。这个模型的一般形式为决策函数：$Y=f(x)$, 或者条件概率分布：$P(Y|X)$.

监督学习方法又可分为生成方法和判别方法。所学到的模型分别称为生成模型和判别模型。

生成方法由数据学习联合概率分布$P(X,y)$，然后求出条件概率分布$P(Y|X)$作为预测模型，即生成模型：

$$P(Y|X)=\frac{P(X,y)}{P(X)}$$

这样的方法称为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。典型的生成模型有：朴素贝叶斯法、隐马尔科夫模型、混合高斯模型、AODE、Latent Dirichlet allocation（unsup）、Restricted Boltzmann Machine。/LDA     

判别方法由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型，即判别模型。判别方法关心的是对给定的输入X，应该预测什么样的输出Y。

### 典型的判别方法包括

线性回归模型、线性判别分析、kNN，感知机，决策树，逻辑回归，最大熵模型，SVM，提升方法，条件随机场，神经网络等。

在监督学习中，生成方法和判别方法各有优缺点，适用于不同条件下的学习问题。    
生成方法的特点：生成方法可以还原联合概率分布，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加的时候，学习的模型可以更快的收敛于真实的模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。     

判别方法的特点：判别方法直接学习的是条件概率或者决策函数，直接面对预测，往往学习的准确率更高；由于直接学习或者，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。

## 优缺点.

1. 一般来说, 生成式模型都会对数据的分布做一定的假设, 比如朴素贝叶斯会假设在给定yy的情况下各个特征之间是条件独立的:$p(X|y)=∏Ni=1p(xi|y)p(X|y)=∏i=1Np(xi|y)$, GDA会假设$p(X|y=c,θ)=N(μc,Σc)p(X|y=c,θ)=N(μc,Σc)$. 当数据满足这些假设时, 生成式模型通常需要较少的数据就能取得不错的效果, 但是当这些假设不成立时, 判别式模型会得到更好的效果.

2. 生成式模型最终得到的错误率会比判别式模型高, 但是其需要更少的训练样本就可以使错误率收敛[限于Genarative-Discriminative Pair, 详见[2]].

3. 生成式模型更容易拟合, 比如在朴素贝叶斯中只需要计下数就可以, 而判别式模型通常都需要解决凸优化问题.

4. 当添加新的类别时, 生成式模型不需要全部重新训练, 只需要计算新的类别ynewynew和xx的联合分布p(ynew,x)p(ynew,x)即可, 而判别式模型则需要全部重新训练.

5. 生成式模型可以更好地利用无标签数据(比如DBN), 而判别式模型不可以.

6. 生成式模型可以生成xx, 因为判别式模型是对p(x,y)p(x,y)进行建模, 这点在DBN的CD算法中中也有体现, 而判别式模型不可以生成xx.

7. 判别式模型可以对输入数据xx进行预处理, 使用ϕ(x)ϕ(x)来代替xx, 如下图所示, 而生成式模型不是很方便进行替换.
