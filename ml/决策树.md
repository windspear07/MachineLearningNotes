

# 1. 概述

1. 决策树是一种基本的分类与回归方法。
2. 决策树模型是描述对样本进行分类的树形结构。树由结点和有向边组成：
    - 内部结点表示一个特征或者属性。
    - 叶子结点表示一个分类。
    - 有向边代表了一个划分规则。
3. 决策树从根结点到子结点的的有向边代表了一条路径。决策树的路径是互斥并且是完备的。
4. 用决策树分类时，对样本的某个特征进行测试，根据测试结果将样本分配到树的子结点上。此时每个子结点对应该特征的一个取值。
    递归地对样本测试，直到该样本被划分叶结点。最后将样本分配为叶结点所属的类。
5. 决策树的优点：可读性强，分类速度快。
6. 决策树学习通常包括3个步骤：
    特征选择。
    决策树生成。
    决策树剪枝。
7. 特点
- 优点：计算复杂度不高，输出结果易于理解，对中间值确实不敏感，可以处理不相关特征数据。
- 确定：容易过拟合
- 适用数据类型：数值、标称

# 2. 基本概念

特征选择的关键是：选取对训练数据有较强分类能力的特征。若一个特征的分类结果与随机分类的结果没有什么差别，则称这个特征是没有分类能力的。

通常特征选择的指标是：信息增益或者信息增益比。这两个指标刻画了特征的分类能力。

## 2.1. 熵

熵是度量样本集合纯度最常用的一种指标，代表一个系统中蕴含多少信息量，信息量越大表明一个系统不确定性就越大，就存在越多的可能性。

假定当前样本集合$D$中第 $k$ 类样本所占的比例为 $p_k(k =1,2,...,|y|)$ ，则 $D$ 的信息熵为：
    
$$ Ent(D) =-\sum_{k=1}^{|y|}p_klog_{2}{p_k} $$ (1)

刻画了样本集中类别的分布情况。

其中，当样本 $D$ 中$|y|$类样本均匀分布时，这时信息熵最大，其值为
    
$$Ent(D) =-\sum_{k=1}^{|y|}\frac{1}{|y|}log_{2}{\frac{1}{|y|}} = \sum_{k=1}^{|y|}\frac{1}{|y|}log_{2}{|y|} = log_{2}{|y|}$$ 
此时样本D的纯度越小；

相反，假设样本D中只有一类样本，此时信息熵最小，其值为

$$Ent(D) =-\sum_{k=1}^{|y|}\frac{1}{|y|}log_{2}{\frac{1}{|y|}} = -1log_21-0log_20-...-0log_20 = 0$$
此时样本的纯度最大。

## 2.2. 熵增益

$$Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent({D^v})$$ (2)
后面是特征a的条件熵，$H(y|A)=\sum_A p(a)H(y|A)$
各个分类的纯度越大越好，则加上负号。

假定在样本D中有某个离散特征 $a$ 有 $V$ 个可能的取值 $(a^1,a^2,...,a^V)$，若使用特征 $a$ 来对样本集 $D$ 进行划分，则会产生$V$个分支结点，其中第$v$个分支结点包含了$D$中所有在特征$a$上取值为$a^v$的样本，样本记为$D^v$，由于根据离散特征a的每个值划分的$V$个分支结点下的样本数量不一致，对于这$V$个分支结点赋予权重$\frac{|D^v|}{|D|}$，即样本数越多的分支结点的影响越大，特征$a$对样本集 $D$ 进行划分所获得的“信息增益”为(2)式。信息增益越大，表示使用特征a来对样本集进行划分所获得的纯度提升越大。

缺点：由于在计算信息增益中倾向于特征值越多的特征进行优先划分，这样假设某个特征值的离散值个数与样本集$D$个数相同（假设为样本编号），虽然用样本编号对样本进行划分，样本纯度提升最高，但是并不具有泛化能力。

## 2.3. 信息增益比$(C4.5)$

$$Gain-ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$$ (3)

基于信息增益的缺点，$C4.5$算法不直接使用信息增益，而是使用一种叫增益比的方法来选择最优特征进行划分，对于样本集$D$中的离散特征$a$，增益率为(3)，其中， 

$$IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}$$

特征a的经验熵。

IV(a) 是特征a的熵。增益率对特征值较少的特征有一定偏好，因此$C4.5$算法选择特征的方法是先从候选特征中选出信息增益高于平均水平的特征，再从这些特征中选择增益率最高的。

# 3. 算法

## 3.1. 基础

1. 构建根结点：将所有训练数据放在根结点。
2. 选择一个最优特征，根据这个特征将训练数据分割成子集，使得各个子集有一个在当前条件下最好的分类。
    - 若这些子集已能够被基本正确分类，则将该子集构成叶结点。
    - 若某个子集不能够被基本正确分类，则对该子集选择新的最优的特征，继续对该子集进行分割，构建相应的结点。
3. 如此递归下去，直至所有训练数据子集都被基本正确分类，或者没有合适的特征为止。

## 3.2. ID3

- 输入： 训练数据集$\mathbb{D}$
- 特征集合: $\mathbb{A}=\{A_1,A_2,\dots,A_n\}$
- 特征信息增益阈值 $\epsilon > 0$ 
- 输出：决策树$T$
- 算法步骤：
    - 若$\mathbb{D}$中所有样本均属于同一类$c_k$，则T为单结点树，并将$c_k$作为该结点的类标记，算法终止。
    - 若$\mathbb{A}=\empty$,则$T$为单结点树，将$\mathbb{D}$中样本数最大的类$c_k$作为该结点的类标记，算法终止。
    - 否则计算$g(\mathbb{D},A_j), j=1,2,\dots,n$，选择信息增益最大的特征$A_g$:
        - 若$g(\mathbb{D},A_g)<\epsilon$，则置$T$为单结点树，将 中样本数最大的类$c_k$作为该结点的类标记，算法终止.
        - 若$g(\mathbb{D},A_g)\ge \epsilon$，则对$A_g$特征的每个可能取值$a_i,i=1,2,\dots,n_g$,根据$A_g=a_i$将$\mathbb{D}$划分为若干个非空子集$\mathbb{D}_i$.
        将$\mathbb{D}_i$中样本数最大的类作为该子集的标记，构建子结点。
    - 对第$i$个子结点，以$\mathbb{D}_i$为训练集， 以$\mathbb{A}-\{A_g\}$为特征集，递归地调用前面的步骤来构建子树。

## 3.3. $C4.5$

生成过程中用信息增益比来选择特征。

## 3.4. 剪枝算法

ID3和C4.5算法的生成树，特别容易过拟合。所以有了剪枝算法。

## 3.5. 基础

1. 决策树生成算法生成的树往往对于训练数据拟合很准确，但是对于未知的测试数据分类却没有那么准确。即出现过拟合现象。
2. 过拟合产生得原因是决策树太复杂。解决的办法是：对决策树剪枝，即对生成的决策树进行简化。
3. 决策树的剪枝是从已生成的树上裁掉一些子树或者叶结点，并将根结点或者其父结点作为新的叶结点。
4. 剪枝的依据是：极小化决策树的整体损失函数或者代价函数。
5. 决策树生成算法是学习局部的模型，决策树剪枝是学习整体的模型。即：生成算法仅考虑局部最优，而剪枝算法考虑全局最优。

### 3.5.1. 理论

todo 补充剪枝算法的公式推导

### 3.5.2. 步骤

- 输入：
    - 生成算法产生的决策树$T$ 
    - 参数$\alpha$ 
- 输出：修剪后的决策树$T$
- 算法步骤：
    - 对树$T$每个结点$T_t$ ，计算其经验熵$H(t)$, 

    $$H(t)=-\sum_{k=1}^K \frac{N_{t,k}}{N_t}log\frac{N_{t,k}}{N_t}$$
    
    - 递归地从树的叶结点向上回退：
    设一组叶结点回退到父结点之前与之后的整棵树分别为$T$与$T'$ ，对应的损失函数值分别为$C_\alpha (T)$与$C_\alpha (T')$ 。若$C_\alpha (T') \le C_\alpha (T)$,则剪枝，将父节点变成叶节点。
    - 递归回退直到不能继续为止，得到损失函数最小的子树

# CART树

1. CART:classfification and regression tree ：学习在给定输入随机变量$\vec x$条件下，输出随机变量$y$的条件概率分布的模型。
    - 它同样由特征选取、树的生成、剪枝组成。
    - 它既可用于分类，也可用于回归。
2. CART 假设决策树是二叉树：
    - 内部结点特征的取值为 是 与 否 。其中：左侧分支取 是，右侧分支取  否 。
    - 它递归地二分每个特征，将输入空间划分为有限个单元。
3. CART 树与ID3 决策树和 C4.5 决策树的重要区别：
    - CART 树是二叉树，而后两者是N叉树。由于是二叉树，因此 CART 树的拆分不依赖于特征的取值数量。因此CART 树也就不像ID3 那样倾向于取值数量较多的特征。
    - CART 树的特征可以是离散的，也可以是连续的.    而后两者的特征是离散的。如果是连续的特征，则需要执行分桶来进行离散化。CART 树处理连续特征时，也可以理解为二分桶的离散化。
4. CART算法分两步：
    - 决策树生成：用训练数据生成尽可能大的决策树。
    - 决策树剪枝：用验证数据基于损失函数最小化的标准对生成的决策树剪枝。

5. 基尼指数

假设有$K$个分类，样本属于第$k$个分类的概率$p_k=p(y=c_k)$，则概率分布的基尼指数

$$Gini(p)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^K p_k^2$$

表示样本集合中，随机一个样本，该样本被分错的概率。gini越小越不容易分错