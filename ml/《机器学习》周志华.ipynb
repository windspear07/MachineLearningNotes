{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绪论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考资料\n",
    "\n",
    "1 [南瓜](https://github.com/datawhalechina/pumpkin-book)\n",
    "\n",
    "2 [notes](https://github.com/Vay-keen/Machine-learning-learning-notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 训练数据是m*d的矩阵，如果有监督值，则为m*q的矩阵\n",
    "\n",
    "2 训练样本中类别的分布也会有影响\n",
    "\n",
    "3 inductive bias归纳偏好\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 nfl的简要描述\n",
    "\n",
    "假设样本空间$\\mathcal{X}$与假设空间$\\mathcal{H}$都是离散的\n",
    "\n",
    "求和范围说明：h是指算法产生的所有假设，$x\\in\\mathcal{X}$是训练集外的数据，f是所有\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\sum_{f}E_{ote}(\\mathfrak{L}a\\vert X,f) =\\sum_f\\sum_h\\sum_{x\\in\\mathcal{X}-X}P(x)\\mathbb{I}(h(x)\\neq f(x))P(h\\vert X,\\mathfrak{L}a)\\\\\n",
    "& =\\sum_{x\\in\\mathcal{X}-X}P(x) \\sum_hP(h\\vert X,\\mathfrak{L}a)\\sum_f\\mathbb{I}(h(x)\\neq f(x))  \\\\\n",
    "& =\\sum_{x\\in\\mathcal{X}-X}P(x) \\sum_hP(h\\vert X,\\mathfrak{L}a)\\cfrac{1}{2}2^{\\vert \\mathcal{X} \\vert} \\\\\n",
    "& =\\cfrac{1}{2}2^{\\vert \\mathcal{X} \\vert}\\sum_{x\\in\\mathcal{X}-X}P(x) \\sum_hP(h\\vert X,\\mathfrak{L}a) \\\\\n",
    "& =2^{\\vert \\mathcal{X} \\vert-1}\\sum_{x\\in\\mathcal{X}-X}P(x) \\cdot \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 经验误差、过拟合\n",
    "\n",
    "1 错误率 $E=\\frac{\\alpha}{m}$\n",
    "\n",
    "2 overfitting and underfitting。过拟合无法彻底解决，因为通常面对的是NP问题，但是有效的学习算法是P，若可彻底解决过拟合，则有N=NP，矛盾。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 评估方法\n",
    "\n",
    "1 hold-out留出法。尽量保持分布一致。一般2/3~4/5用于训练。\n",
    "\n",
    "2 cross validation，交叉验证。\n",
    "\n",
    "3 bootstraping自助法，放回取样。自助法在数据集较小、难以有效划分训练/测试集时很有用;此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处.然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差.\n",
    "\n",
    "每个样本始终不被采集到的几率\n",
    "\n",
    "$$\\lim_{n\\to +\\infty}(1-\\frac{1}{m})^m=\\frac{1}{e} \\approx 0.378$$\n",
    "\n",
    "4 参数与最终模型。调参和算法选择没什么本质区别，但是连续区间的参数需要以步长去选择。sklearn里的grid search就是做这个的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 perfermance measure\n",
    "\n",
    "1 衡量的标准，最常见的均方差mean square error：\n",
    "\n",
    "$$E(f;D)=\\frac{1}{m}\\sum_{i=1}^m (f(x_i)-y_i)^2$$\n",
    "\n",
    "2 下面说明分类任务常用的度量方法。\n",
    "\n",
    "3 错误率\n",
    "\n",
    "$$E(f;D)=\\frac{1}{m}\\sum_{i=1}^m \\mathbb{I}(f(x_i)\\neq y_i)$$\n",
    "\n",
    "精度\n",
    "\n",
    "$$acc(f;D)=\\frac{1}{m}\\sum_{i=1}^m \\mathbb{I}(f(x_i) = y_i)$$\n",
    "\n",
    "4 查准率、查全率\n",
    "\n",
    "| 真实情况 | 预测正例 | 预测反例 |\n",
    "| ------ | ------ | ------ |\n",
    "| 正例 | TP真正例 | FN假反例 |\n",
    "| 反例 | FP假正例 | TN真反例 |\n",
    "\n",
    "查准率 $P=\\frac{TP}{TP+FP}$,就是说预测正例里面有多少真正的正例。\n",
    "\n",
    "查全率 $R=\\frac{TP}{TP+FN}$，就是说实际上所有的正例，有多少被预测出来了，全不全。\n",
    "\n",
    "5 了查准率 查全率曲线，简称 P-R曲线，\n",
    "\n",
    "6 F1调和平均 $\\frac{1}{F1}=\\frac{1}{P}+\\frac{1}{R}$\n",
    "\n",
    "每次分类会得到一个混淆矩阵\n",
    "\n",
    "7 ROC与AUC。截断点的不同，可以控制查准率、查全率。threshold\n",
    "\n",
    "ROC是receiver operating characteristic受试者工作特征。纵横分别为真正率true positive rate、假正率false positive rate。\n",
    "\n",
    "$$TPR=\\frac{TP}{TP+FN}$$\n",
    "\n",
    "$$FPR=\\frac{FP}{TN+FP}$$\n",
    "\n",
    "AUC area under roc curve.\n",
    "\n",
    "8 代价敏感率与代价曲线\n",
    "\n",
    "cost-sensitive错误率为\n",
    "\n",
    "$$E(f;D;cost)=\\frac{1}{m}(\\sum_{x_i \\in D+} \\mathbb{I}(f(x_i)\\neq y_i)*cost_{01}  + \\sum_{x_i \\in D-}^m \\mathbb{I}(f(x_i)\\neq y_i)*cost_{10} ) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 比较检验 #todo\n",
    "\n",
    "1 不好比较。我们想要泛化性能，和测试性能不同；测试的性能与测试集相关；有的算法具有随机性。\n",
    "\n",
    "2 统计假设检验hypothesis test是理论基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 偏差与方差\n",
    "\n",
    "1 bias-variance decomposition，偏差-方差分解是解释学习算法泛化性能的一种重要工具。\n",
    "\n",
    "学习算法的期望预测 $\\bar{f} (x)=\\mathbb{E}_D[f(x;D)]$\n",
    "\n",
    "使用样本数相同的不同训练集产生的方差 $var(x)=\\mathbb{E}_D[(f(x;D)-\\bar{f}(x))^2]$\n",
    "\n",
    "噪声 $\\epsilon^2 = \\mathbb{E}_D[(y_D-y)^2]$\n",
    "\n",
    "经过计算有\n",
    "\n",
    "$$E(f;D)=bias^2(x) + var(x) + \\epsilon^2$$\n",
    "\n",
    "$bias^2$度量拟合能力\n",
    "\n",
    "偏差一方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的.给定\n",
    "学习任务为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 线性模型\n",
    "\n",
    "1 基本形式 $f(x)=w^T x +b$\n",
    "\n",
    "2 线性回归\n",
    "\n",
    "目标函数\n",
    "\n",
    "$$(w^*,b^*)= argmin_{(w,b)}\\sum_{i=1}^m (y_i-wx_i-b)^2$$\n",
    "\n",
    "其偏导数\n",
    "\n",
    "$$\\frac{\\partial E(w,b)}{\\partial w}=2(w\\sum_{i=1}^m x_i^2 - \\sum_{i=1}^m (y_i-b)x_i)$$\n",
    "\n",
    "\n",
    "解得\n",
    "$$ w=\\cfrac{\\sum_{i=1}^{m}y_i(x_i-\\bar{x})}{\\sum_{i=1}^{m}x_i^2-\\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2} $$\n",
    "\n",
    "\n",
    "[推导]：令式（3.5）等于0： \n",
    "\n",
    "$$ 0 = w\\sum_{i=1}^{m}x_i^2-\\sum_{i=1}^{m}(y_i-b)x_i $$ \n",
    "$$ w\\sum_{i=1}^{m}x_i^2 = \\sum_{i=1}^{m}y_ix_i-\\sum_{i=1}^{m}bx_i $$ \n",
    "\n",
    "由于令式（3.6）等于0可得$ b=\\cfrac{1}{m}\\sum_{i=1}^{m}(y_i-wx_i) $，又$ \\cfrac{1}{m}\\sum_{i=1}^{m}y_i=\\bar{y} $，$ \\cfrac{1}{m}\\sum_{i=1}^{m}x_i=\\bar{x} $，则$ b=\\bar{y}-w\\bar{x} $，代入上式可得： $$ \\begin{aligned} w\\sum_{i=1}^{m}x_i^2 & = \\sum_{i=1}^{m}y_ix_i-\\sum_{i=1}^{m}(\\bar{y}-w\\bar{x})x_i \\ w\\sum_{i=1}^{m}x_i^2 & = \\sum_{i=1}^{m}y_ix_i-\\bar{y}\\sum_{i=1}^{m}x_i+w\\bar{x}\\sum_{i=1}^{m}x_i \\ w(\\sum_{i=1}^{m}x_i^2-\\bar{x}\\sum_{i=1}^{m}x_i) & = \\sum_{i=1}^{m}y_ix_i-\\bar{y}\\sum_{i=1}^{m}x_i \\ w & = \\cfrac{\\sum_{i=1}^{m}y_ix_i-\\bar{y}\\sum_{i=1}^{m}x_i}{\\sum_{i=1}^{m}x_i^2-\\bar{x}\\sum_{i=1}^{m}x_i} \\end{aligned} $$ 又$ \\bar{y}\\sum_{i=1}^{m}x_i=\\cfrac{1}{m}\\sum_{i=1}^{m}y_i\\sum_{i=1}^{m}x_i=\\bar{x}\\sum_{i=1}^{m}y_i $，$ \\bar{x}\\sum_{i=1}^{m}x_i=\\cfrac{1}{m}\\sum_{i=1}^{m}x_i\\sum_{i=1}^{m}x_i=\\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2 $，代入上式即可得式（3.7）： $$ w=\\cfrac{\\sum_{i=1}^{m}y_i(x_i-\\bar{x})}{\\sum_{i=1}^{m}x_i^2-\\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2} $$\n",
    "\n",
    "\n",
    "【注】：式（3.7）还可以进一步化简为能用向量表达的形式，将$ \\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2=\\bar{x}\\sum_{i=1}^{m}x_i $代入分母可得： $$ \\begin{aligned}\n",
    "w & = \\cfrac{\\sum_{i=1}^{m}y_i(x_i-\\bar{x})}{\\sum_{i=1}^{m}x_i^2-\\bar{x}\\sum_{i=1}^{m}x_i} \\ & = \\cfrac{\\sum_{i=1}^{m}(y_ix_i-y_i\\bar{x})}{\\sum_{i=1}^{m}(x_i^2-x_i\\bar{x})} \\end{aligned} $$ 又因为$ \\bar{y}\\sum_{i=1}^{m}x_i=\\bar{x}\\sum_{i=1}^{m}y_i=\\sum_{i=1}^{m}\\bar{y}x_i=\\sum_{i=1}^{m}\\bar{x}y_i=m\\bar{x}\\bar{y}=\\sum_{i=1}^{m}\\bar{x}\\bar{y} $，$\\sum_{i=1}^{m}x_i\\bar{x}=\\bar{x}\\sum_{i=1}^{m}x_i=\\bar{x}\\cdot m \\cdot\\frac{1}{m}\\cdot\\sum_{i=1}^{m}x_i=m\\bar{x}^2=\\sum_{i=1}^{m}\\bar{x}^2$，则上式可化为： $$ \\begin{aligned} w & = \\cfrac{\\sum_{i=1}^{m}(y_ix_i-y_i\\bar{x}-x_i\\bar{y}+\\bar{x}\\bar{y})}{\\sum_{i=1}^{m}(x_i^2-x_i\\bar{x}-x_i\\bar{x}+\\bar{x}^2)} \\ & = \\cfrac{\\sum_{i=1}^{m}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{m}(x_i-\\bar{x})^2} \\end{aligned} $$ 若令$ \\boldsymbol{x}=(x_1,x_2,...,x_m)^T $，$ \\boldsymbol{x}{d} $为去均值后的$ \\boldsymbol{x} $，$ \\boldsymbol{y}=(y_1,y_2,...,y_m)^T $，$ \\boldsymbol{y}{d} $为去均值后的$ \\boldsymbol{y} $，其中$ \\boldsymbol{x} $、$ \\boldsymbol{x}{d} $、$ \\boldsymbol{y} $、$ \\boldsymbol{y}{d} $均为m行1列的列向量，代入上式可得： $$ w=\\cfrac{\\boldsymbol{x}{d}^T\\boldsymbol{y}{d}}{\\boldsymbol{x}d^T\\boldsymbol{x}{d}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 对数几率回归 \n",
    "\n",
    "$$y=\\frac{1}{1+e^{-z}} $$\n",
    "\n",
    "$ln \\frac{y}{1-y}=z$,其中y是正例，1-y是反例，俗称几率。\n",
    "\n",
    "采用极大似然法确定参数w、b。\n",
    "\n",
    "对于数据集${(x_i,y_i)}_{i=1}^m$，对率回归模型最大化对数似然\n",
    "\n",
    "$$\\mathbb(l)(w,b)=\\sum_{i=1}^m ln p(y_i | x_i;w,b)$$\n",
    "\n",
    "即令每个样本术语真实标记的概率越大越好。令$\\beta =(w;b), \\hat{x}=(x;1),则w^Tx+b=\\beta^T \\hat x$\n",
    "\n",
    "#todo 补充andrew的推导部分\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 优化方法\n",
    "\n",
    "(1)[牛顿迭代解方程](https://matongxue.com/madocs/205.html)\n",
    "\n",
    "曲线方程$f(x)$，我们在$x_n$点做切线，求$x_{n+1}$。在xn的切线方程为$y=f(x_n)+f^{'}(x_n)(x-x_n)$,令y=0，得\n",
    "\n",
    "$$x_{n+1}=x_n-f(x_n)/f^{'}(x_n)$$\n",
    "\n",
    "(2) [牛顿法优化](https://blog.csdn.net/sigai_csdn/article/details/80678812)\n",
    "\n",
    "忽略二次项的泰勒展开式 \n",
    "\n",
    "$$f(x)=f(x_0) + f^{'}(x_0)(x-x_0)+f^{\"}(x-x_0)^2$$\n",
    "\n",
    "现在在x0处，需要以此为基础找到导数0的点，对上式求导，令导数为零得\n",
    "\n",
    "$$f^{'}(x)=f^{'}(x_0)+f^{\"}(x_0)(x-x_0)=0$$\n",
    "\n",
    "得 $x=x_0-\\frac{f^{'}(x_0)}{f^{\"}(x_0)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 (Linear Discriminant analys )LDA线性判别分析\n",
    "\n",
    "$$J=\\cfrac{\\boldsymbol w^T(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^T\\boldsymbol w}{\\boldsymbol w^T(\\Sigma_0+\\Sigma_1)\\boldsymbol w}$$\n",
    "\n",
    "[推导]： \n",
    "$$\\begin{aligned} J = \\cfrac{\\big|\\big|\\boldsymbol w^T\\mu_0-\\boldsymbol w^T\\mu_1\\big|\\big|_2^2}{\\boldsymbol w^T(\\Sigma_0+\\Sigma_1)\\boldsymbol w} \\\\ \n",
    "&= \\cfrac{\\big|\\big|(\\boldsymbol w^T\\mu_0-\\boldsymbol w^T\\mu_1)^T\\big|\\big|_2^2}{\\boldsymbol w^T(\\Sigma_0+\\Sigma_1)\\boldsymbol w} \\\\ \n",
    "&= \\cfrac{\\big|\\big|(\\mu_0-\\mu_1)^T\\boldsymbol w\\big|\\big|_2^2}{\\boldsymbol w^T(\\Sigma_0+\\Sigma_1)\\boldsymbol w} \\\\ \n",
    "&= \\cfrac{[(\\mu_0-\\mu_1)^T\\boldsymbol w]^T(\\mu_0-\\mu_1)^T\\boldsymbol w}{\\boldsymbol w^T(\\Sigma_0+\\Sigma_1)\\boldsymbol w} \\\\ \n",
    "&= \\cfrac{\\boldsymbol w^T(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^T\\boldsymbol w}{\\boldsymbol w^T(\\Sigma_0+\\Sigma_1)\\boldsymbol w} \n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7 [拉格朗日乘子法](https://matongxue.com/madocs/939/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
