{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绪论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考资料\n",
    "\n",
    "1 [南瓜](https://github.com/datawhalechina/pumpkin-book)\n",
    "\n",
    "2 [notes](https://github.com/Vay-keen/Machine-learning-learning-notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 训练数据是m*d的矩阵，如果有监督值，则为m*q的矩阵\n",
    "\n",
    "2 训练样本中类别的分布也会有影响\n",
    "\n",
    "3 inductive bias归纳偏好\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 nfl的简要描述\n",
    "\n",
    "假设样本空间$\\mathcal{X}$与假设空间$\\mathcal{H}$都是离散的\n",
    "\n",
    "求和范围说明：h是指算法产生的所有假设，$x\\in\\mathcal{X}$是训练集外的数据，f是所有\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\sum_{f}E_{ote}(\\mathfrak{L}a\\vert X,f) =\\sum_f\\sum_h\\sum_{x\\in\\mathcal{X}-X}P(x)\\mathbb{I}(h(x)\\neq f(x))P(h\\vert X,\\mathfrak{L}a)\\\\\n",
    "& =\\sum_{x\\in\\mathcal{X}-X}P(x) \\sum_hP(h\\vert X,\\mathfrak{L}a)\\sum_f\\mathbb{I}(h(x)\\neq f(x))  \\\\\n",
    "& =\\sum_{x\\in\\mathcal{X}-X}P(x) \\sum_hP(h\\vert X,\\mathfrak{L}a)\\cfrac{1}{2}2^{\\vert \\mathcal{X} \\vert} \\\\\n",
    "& =\\cfrac{1}{2}2^{\\vert \\mathcal{X} \\vert}\\sum_{x\\in\\mathcal{X}-X}P(x) \\sum_hP(h\\vert X,\\mathfrak{L}a) \\\\\n",
    "& =2^{\\vert \\mathcal{X} \\vert-1}\\sum_{x\\in\\mathcal{X}-X}P(x) \\cdot \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 经验误差、过拟合\n",
    "\n",
    "1 错误率 $E=\\frac{\\alpha}{m}$\n",
    "\n",
    "2 overfitting and underfitting。过拟合无法彻底解决，因为通常面对的是NP问题，但是有效的学习算法是P，若可彻底解决过拟合，则有N=NP，矛盾。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 评估方法\n",
    "\n",
    "1 hold-out留出法。尽量保持分布一致。一般2/3~4/5用于训练。\n",
    "\n",
    "2 cross validation，交叉验证。\n",
    "\n",
    "3 bootstraping自助法，放回取样。自助法在数据集较小、难以有效划分训练/测试集时很有用;此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处.然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差.\n",
    "\n",
    "每个样本始终不被采集到的几率\n",
    "\n",
    "$$\\lim_{n\\to +\\infty}(1-\\frac{1}{m})^m=\\frac{1}{e} \\approx 0.378$$\n",
    "\n",
    "4 参数与最终模型。调参和算法选择没什么本质区别，但是连续区间的参数需要以步长去选择。sklearn里的grid search就是做这个的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 perfermance measure\n",
    "\n",
    "1 衡量的标准，最常见的均方差mean square error：\n",
    "\n",
    "$$E(f;D)=\\frac{1}{m}\\sum_{i=1}^m (f(x_i)-y_i)^2$$\n",
    "\n",
    "2 下面说明分类任务常用的度量方法。\n",
    "\n",
    "3 错误率\n",
    "\n",
    "$$E(f;D)=\\frac{1}{m}\\sum_{i=1}^m \\mathbb{I}(f(x_i)\\neq y_i)$$\n",
    "\n",
    "精度\n",
    "\n",
    "$$acc(f;D)=\\frac{1}{m}\\sum_{i=1}^m \\mathbb{I}(f(x_i) = y_i)$$\n",
    "\n",
    "4 查准率、查全率\n",
    "\n",
    "| 真实情况 | 预测正例 | 预测反例 |\n",
    "| ------ | ------ | ------ |\n",
    "| 正例 | TP真正例 | FN假反例 |\n",
    "| 反例 | FP假正例 | TN真反例 |\n",
    "\n",
    "查准率 $P=\\frac{TP}{TP+FP}$,就是说预测正例里面有多少真正的正例。\n",
    "\n",
    "查全率 $R=\\frac{TP}{TP+FN}$，就是说实际上所有的正例，有多少被预测出来了，全不全。\n",
    "\n",
    "5 了查准率 查全率曲线，简称 P-R曲线，\n",
    "\n",
    "6 F1调和平均 $\\frac{1}{F1}=\\frac{1}{P}+\\frac{1}{R}$\n",
    "\n",
    "每次分类会得到一个混淆矩阵\n",
    "\n",
    "7 ROC与AUC。截断点的不同，可以控制查准率、查全率。threshold\n",
    "\n",
    "ROC是receiver operating characteristic受试者工作特征。纵横分别为真正率true positive rate、假正率false positive rate。\n",
    "\n",
    "$$TPR=\\frac{TP}{TP+FN}$$\n",
    "\n",
    "$$FPR=\\frac{FP}{TN+FP}$$\n",
    "\n",
    "AUC area under roc curve.\n",
    "\n",
    "8 代价敏感率与代价曲线\n",
    "\n",
    "cost-sensitive错误率为\n",
    "\n",
    "$$E(f;D;cost)=\\frac{1}{m}(\\sum_{x_i \\in D+} \\mathbb{I}(f(x_i)\\neq y_i)*cost_{01}  + \\sum_{x_i \\in D-}^m \\mathbb{I}(f(x_i)\\neq y_i)*cost_{10} ) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 比较检验 #todo\n",
    "\n",
    "1 不好比较。我们想要泛化性能，和测试性能不同；测试的性能与测试集相关；有的算法具有随机性。\n",
    "\n",
    "2 统计假设检验hypothesis test是理论基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 偏差与方差\n",
    "\n",
    "1 bias-variance decomposition，偏差-方差分解是解释学习算法泛化性能的一种重要工具。\n",
    "\n",
    "学习算法的期望预测 $\\bar{f} (x)=\\mathbb{E}_D[f(x;D)]$\n",
    "\n",
    "使用样本数相同的不同训练集产生的方差 $var(x)=\\mathbb{E}_D[(f(x;D)-\\bar{f}(x))^2]$\n",
    "\n",
    "噪声 $\\epsilon^2 = \\mathbb{E}_D[(y_D-y)^2]$\n",
    "\n",
    "经过计算有\n",
    "\n",
    "$$E(f;D)=bias^2(x) + var(x) + \\epsilon^2$$\n",
    "\n",
    "$bias^2$度量拟合能力\n",
    "\n",
    "偏差一方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的.给定\n",
    "学习任务为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 线性模型\n",
    "\n",
    "1 基本形式 $f(x)=w^T x +b$\n",
    "\n",
    "2 线性回归\n",
    "\n",
    "目标函数\n",
    "\n",
    "$$(w^*,b^*)= argmin_{(w,b)}\\sum_{i=1}^m (y_i-wx_i-b)^2$$\n",
    "\n",
    "其偏导数\n",
    "\n",
    "$$\\frac{\\partial E(w,b)}{\\partial w}=2(w\\sum_{i=1}^m x_i^2 - \\sum_{i=1}^m (y_i-b)x_i)$$\n",
    "\n",
    "\n",
    "解得\n",
    "$$ w=\\cfrac{\\sum_{i=1}^{m}y_i(x_i-\\bar{x})}{\\sum_{i=1}^{m}x_i^2-\\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2} $$\n",
    "\n",
    "\n",
    "[推导]：令式（3.5）等于0： \n",
    "\n",
    "$$ 0 = w\\sum_{i=1}^{m}x_i^2-\\sum_{i=1}^{m}(y_i-b)x_i $$ \n",
    "$$ w\\sum_{i=1}^{m}x_i^2 = \\sum_{i=1}^{m}y_ix_i-\\sum_{i=1}^{m}bx_i $$ \n",
    "\n",
    "由于令式（3.6）等于0可得$ b=\\cfrac{1}{m}\\sum_{i=1}^{m}(y_i-wx_i) $，又$ \\cfrac{1}{m}\\sum_{i=1}^{m}y_i=\\bar{y} $，$ \\cfrac{1}{m}\\sum_{i=1}^{m}x_i=\\bar{x} $，则$ b=\\bar{y}-w\\bar{x} $，代入上式可得： $$ \\begin{aligned} w\\sum_{i=1}^{m}x_i^2 & = \\sum_{i=1}^{m}y_ix_i-\\sum_{i=1}^{m}(\\bar{y}-w\\bar{x})x_i \\ w\\sum_{i=1}^{m}x_i^2 & = \\sum_{i=1}^{m}y_ix_i-\\bar{y}\\sum_{i=1}^{m}x_i+w\\bar{x}\\sum_{i=1}^{m}x_i \\ w(\\sum_{i=1}^{m}x_i^2-\\bar{x}\\sum_{i=1}^{m}x_i) & = \\sum_{i=1}^{m}y_ix_i-\\bar{y}\\sum_{i=1}^{m}x_i \\ w & = \\cfrac{\\sum_{i=1}^{m}y_ix_i-\\bar{y}\\sum_{i=1}^{m}x_i}{\\sum_{i=1}^{m}x_i^2-\\bar{x}\\sum_{i=1}^{m}x_i} \\end{aligned} $$ 又$ \\bar{y}\\sum_{i=1}^{m}x_i=\\cfrac{1}{m}\\sum_{i=1}^{m}y_i\\sum_{i=1}^{m}x_i=\\bar{x}\\sum_{i=1}^{m}y_i $，$ \\bar{x}\\sum_{i=1}^{m}x_i=\\cfrac{1}{m}\\sum_{i=1}^{m}x_i\\sum_{i=1}^{m}x_i=\\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2 $，代入上式即可得式（3.7）： $$ w=\\cfrac{\\sum_{i=1}^{m}y_i(x_i-\\bar{x})}{\\sum_{i=1}^{m}x_i^2-\\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2} $$\n",
    "\n",
    "\n",
    "【注】：式（3.7）还可以进一步化简为能用向量表达的形式，将$ \\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2=\\bar{x}\\sum_{i=1}^{m}x_i $代入分母可得： $$ \\begin{aligned}\n",
    "w & = \\cfrac{\\sum_{i=1}^{m}y_i(x_i-\\bar{x})}{\\sum_{i=1}^{m}x_i^2-\\bar{x}\\sum_{i=1}^{m}x_i} \\ & = \\cfrac{\\sum_{i=1}^{m}(y_ix_i-y_i\\bar{x})}{\\sum_{i=1}^{m}(x_i^2-x_i\\bar{x})} \\end{aligned} $$ 又因为$ \\bar{y}\\sum_{i=1}^{m}x_i=\\bar{x}\\sum_{i=1}^{m}y_i=\\sum_{i=1}^{m}\\bar{y}x_i=\\sum_{i=1}^{m}\\bar{x}y_i=m\\bar{x}\\bar{y}=\\sum_{i=1}^{m}\\bar{x}\\bar{y} $，$\\sum_{i=1}^{m}x_i\\bar{x}=\\bar{x}\\sum_{i=1}^{m}x_i=\\bar{x}\\cdot m \\cdot\\frac{1}{m}\\cdot\\sum_{i=1}^{m}x_i=m\\bar{x}^2=\\sum_{i=1}^{m}\\bar{x}^2$，则上式可化为： $$ \\begin{aligned} w & = \\cfrac{\\sum_{i=1}^{m}(y_ix_i-y_i\\bar{x}-x_i\\bar{y}+\\bar{x}\\bar{y})}{\\sum_{i=1}^{m}(x_i^2-x_i\\bar{x}-x_i\\bar{x}+\\bar{x}^2)} \\ & = \\cfrac{\\sum_{i=1}^{m}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{m}(x_i-\\bar{x})^2} \\end{aligned} $$ 若令$ \\boldsymbol{x}=(x_1,x_2,...,x_m)^T $，$ \\boldsymbol{x}{d} $为去均值后的$ \\boldsymbol{x} $，$ \\boldsymbol{y}=(y_1,y_2,...,y_m)^T $，$ \\boldsymbol{y}{d} $为去均值后的$ \\boldsymbol{y} $，其中$ \\boldsymbol{x} $、$ \\boldsymbol{x}{d} $、$ \\boldsymbol{y} $、$ \\boldsymbol{y}{d} $均为m行1列的列向量，代入上式可得： $$ w=\\cfrac{\\boldsymbol{x}{d}^T\\boldsymbol{y}{d}}{\\boldsymbol{x}d^T\\boldsymbol{x}{d}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 对数几率回归 \n",
    "\n",
    "$$y=\\frac{1}{1+e^{-z}} $$\n",
    "\n",
    "$ln \\frac{y}{1-y}=z$,其中y是正例，1-y是反例，俗称几率。\n",
    "\n",
    "采用极大似然法确定参数w、b。\n",
    "\n",
    "对于数据集${(x_i,y_i)}_{i=1}^m$，对率回归模型最大化对数似然\n",
    "\n",
    "$$\\mathbb(l)(w,b)=\\sum_{i=1}^m ln p(y_i | x_i;w,b)$$\n",
    "\n",
    "即令每个样本术语真实标记的概率越大越好。令$\\beta =(w;b), \\hat{x}=(x;1),则w^Tx+b=\\beta^T \\hat x$\n",
    "\n",
    "#todo 补充andrew的推导部分\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 优化方法\n",
    "\n",
    "(1)[牛顿迭代解方程](https://matongxue.com/madocs/205.html)\n",
    "\n",
    "注意，这个不是优化方法，只是列在这里对比！\n",
    "\n",
    "曲线方程$f(x)$，我们在$x_n$点做切线，求$x_{n+1}$。在xn的切线方程为$y=f(x_n)+f^{'}(x_n)(x-x_n)$,令y=0，得\n",
    "\n",
    "$$x_{n+1}=x_n-f(x_n)/f^{'}(x_n)$$\n",
    "\n",
    "(2) [牛顿法优化](https://blog.csdn.net/sigai_csdn/article/details/80678812)\n",
    "\n",
    "忽略二次项的泰勒展开式 \n",
    "\n",
    "$$f(x)=f(x_0) + f^{'}(x_0)(x-x_0)+f^{\"}(x-x_0)^2$$\n",
    "\n",
    "现在在x0处，需要以此为基础找到导数0的点，对上式求导，令导数为零得\n",
    "\n",
    "$$f^{'}(x)=f^{'}(x_0)+f^{\"}(x_0)(x-x_0)=0$$\n",
    "\n",
    "得 $x=x_0-\\frac{f^{'}(x_0)}{f^{\"}(x_0)}$\n",
    "\n",
    "(3)Batch Gradient Descent （BGD）\n",
    "\n",
    "[ref1](https://www.cnblogs.com/guoyaohua/p/8542554.html)\n",
    "[ref2](https://arxiv.org/pdf/1609.04747.pdf)采用整个训练集的数据来计算 cost function 对参数的梯度，有\n",
    "\n",
    "$\\theta = \\theta - \\eta \\nabla J(\\theta) $\n",
    "\n",
    "由于这种方法是在一次更新中，就对整个数据集计算梯度，所以计算起来非常慢，遇到很大量的数据集也会非常棘手，而且不能投入新数据实时更新模型。\n",
    "\n",
    "(4)Stochastic Gradient Descent  sgd\n",
    "\n",
    "和 BGD 的一次用所有数据计算梯度相比，SGD 每次更新时对每个样本进行梯度更新，对于很大的数据集来说，可能会有相似的样本，这样 BGD 在计算梯度时会出现冗余，而 SGD 一次只进行一次更新，就没有冗余，而且比较快，并且可以新增样本。\n",
    "\n",
    "$\\theta = \\theta - \\eta \\nabla J(\\theta ; x^{(i)},y^{(i)}) $\n",
    "\n",
    "SGD 因为更新比较频繁，会造成 cost function 有严重的震荡。BGD 可以收敛到局部极小值，当然 SGD 的震荡可能会跳到更好的局部极小值处。\n",
    "当我们稍微减小 learning rate，SGD 和 BGD 的收敛性是一样的。\n",
    "\n",
    "(5)Mini-Batch Gradient Descent （MBGD）\n",
    "\n",
    "MBGD 每一次利用一小批样本，即 n 个样本进行计算，这样它可以降低参数更新时的方差，收敛更稳定，另一方面可以充分地利用深度学习库中高度优化的矩阵操作来进行更有效的梯度计算。\n",
    "\n",
    "$\\theta = \\theta - \\eta \\nabla J(\\theta ; x^{(i;i+n)},y^{(i;i+n)}) $\n",
    "\n",
    "超参数设定值:  n 一般取值在 50～256\n",
    "\n",
    "缺点：Mini-batch gradient descent 不能保证很好的收敛性，learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离。（有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点。）对于非凸函数，还要避免陷于局部极小值处，或者鞍点处，因为鞍点周围的error是一样的，所有维度的梯度都接近于0，SGD 很容易被困在这里。（会在鞍点或者局部最小点震荡跳动，因为在此点处，如果是训练集全集带入即BGD，则优化会停止不动，如果是mini-batch或者SGD，每次找到的梯度都是不同的，就会发生震荡，来回跳动。）SGD对所有参数更新时应用同样的 learning rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新。LR会随着更新的次数逐渐变小。\n",
    "\n",
    "(6)Momentum\n",
    "\n",
    "SGD 在 ravines 的情况下容易被困住， ravines 就是曲面的一个方向比另一个方向更陡，这时 SGD 会发生震荡而迟迟不能接近极小值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 (Linear Discriminant analys )LDA线性判别分析\n",
    "\n",
    "$$J=\\cfrac{\\boldsymbol w^T(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^T\\boldsymbol w}{\\boldsymbol w^T(\\Sigma_0+\\Sigma_1)\\boldsymbol w}$$\n",
    "\n",
    "[推导]： \n",
    "$$\\begin{aligned} J = \\cfrac{\\big|\\big|\\boldsymbol w^T\\mu_0-\\boldsymbol w^T\\mu_1\\big|\\big|_2^2}{\\boldsymbol w^T(\\Sigma_0+\\Sigma_1)\\boldsymbol w} \\\\ \n",
    "&= \\cfrac{\\big|\\big|(\\boldsymbol w^T\\mu_0-\\boldsymbol w^T\\mu_1)^T\\big|\\big|_2^2}{\\boldsymbol w^T(\\Sigma_0+\\Sigma_1)\\boldsymbol w} \\\\ \n",
    "&= \\cfrac{\\big|\\big|(\\mu_0-\\mu_1)^T\\boldsymbol w\\big|\\big|_2^2}{\\boldsymbol w^T(\\Sigma_0+\\Sigma_1)\\boldsymbol w} \\\\ \n",
    "&= \\cfrac{[(\\mu_0-\\mu_1)^T\\boldsymbol w]^T(\\mu_0-\\mu_1)^T\\boldsymbol w}{\\boldsymbol w^T(\\Sigma_0+\\Sigma_1)\\boldsymbol w} \\\\ \n",
    "&= \\cfrac{\\boldsymbol w^T(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^T\\boldsymbol w}{\\boldsymbol w^T(\\Sigma_0+\\Sigma_1)\\boldsymbol w} \n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 [拉格朗日乘子法](https://matongxue.com/madocs/939/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 决策树\n",
    "\n",
    "1 信息熵。熵是度量样本集合纯度最常用的一种指标，代表一个系统中蕴含多少信息量，信息量越大表明一个系统不确定性就越大，就存在越多的可能性。\n",
    "\n",
    "假定当前样本集合 $D$ 中第 $k$ 类样本所占的比例为 $p_k(k =1,2,...,|y|)$ ，则 $D$ 的信息熵为：\n",
    "\n",
    "$$ Ent(D) =-\\sum_{k=1}^{|y|}p_klog_{2}{p_k} $$\n",
    "\n",
    "其中，当样本 $D$ 中 $|y|$ 类样本均匀分布时，这时信息熵最大，其值为 $$ Ent(D) =-\\sum_{k=1}^{|y|}\\frac{1}{|y|}log_{2}{\\frac{1}{|y|}} = \\sum_{k=1}^{|y|}\\frac{1}{|y|}log_{2}{|y|} = log_{2}{|y|} $$ 此时样本D的纯度越小；\n",
    "\n",
    "相反，假设样本D中只有一类样本，此时信息熵最小，其值为 $$ Ent(D) =-\\sum_{k=1}^{|y|}\\frac{1}{|y|}log_{2}{\\frac{1}{|y|}} = -1log_21-0log_20-...-0log_20 = 0 $$ 此时样本的纯度最大。\n",
    "\n",
    "2 熵增益。$$ Gain(D,a) = Ent(D) - \\sum_{v=1}^{V}\\frac{|D^v|}{|D|}Ent({D^v}) $$ [解析]：假定在样本D中有某个离散特征 $a$ 有 $V$ 个可能的取值 $(a^1,a^2,...,a^V)$，若使用特征 $a$ 来对样本集 $D$ 进行划分，则会产生 $V$ 个分支结点，其中第 $v$ 个分支结点包含了 $D$ 中所有在特征 $a$ 上取值为 $a^v$ 的样本，样本记为 $D^v$，由于根据离散特征a的每个值划分的 $V$ 个分支结点下的样本数量不一致，对于这 $V$ 个分支结点赋予权重 $\\frac{|D^v|}{|D|}$，即样本数越多的分支结点的影响越大，特征 $a$ 对样本集 $D$ 进行划分所获得的“信息增益”为 $$ Gain(D,a) = Ent(D) - \\sum_{v=1}^{V}\\frac{|D^v|}{|D|}Ent({D^v}) $$ 信息增益越大，表示使用特征a来对样本集进行划分所获得的纯度提升越大。\n",
    "\n",
    "缺点：由于在计算信息增益中倾向于特征值越多的特征进行优先划分，这样假设某个特征值的离散值个数与样本集 $D$ 个数相同（假设为样本编号），虽然用样本编号对样本进行划分，样本纯度提升最高，但是并不具有泛化能力。\n",
    "\n",
    "3 $$ Gain-ratio(D,a)=\\frac{Gain(D,a)}{IV(a)} $$ [解析]：基于信息增益的缺点，$C4.5$ 算法不直接使用信息增益，而是使用一种叫增益率的方法来选择最优特征进行划分，对于样本集 $D$ 中的离散特征 $a$ ，增益率为 $$ Gain-ratio(D,a)=\\frac{Gain(D,a)}{IV(a)} $$ 其中， $$ IV(a)=-\\sum_{v=1}^{V}\\frac{|D^v|}{|D|}log_2\\frac{|D^v|}{|D|} $$ IV(a) 是特征 a 的熵。\n",
    "\n",
    "增益率对特征值较少的特征有一定偏好，因此 $C4.5$ 算法选择特征的方法是先从候选特征中选出信息增益高于平均水平的特征，再从这些特征中选择增益率最高的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 神经网络\n",
    "\n",
    "$$\\Delta w_i = \\eta(y-\\hat{y})x_i$$ [推导]：此处感知机的模型为： $$y=f(\\sum_{i} w_i x_i - \\theta)$$ 将$\\theta$看成哑结点后，模型可化简为： $$y=f(\\sum_{i} w_i x_i)=f(\\boldsymbol w^T \\boldsymbol x)$$ 其中$f$为阶跃函数。\n",
    "根据《统计学习方法》§2可知，假设误分类点集合为$M$，$\\boldsymbol x_i \\in M$为误分类点，$\\boldsymbol x_i$的真实标签为$y_i$,模型的预测值为$\\hat{y_i}$,对于误分类点$\\boldsymbol x_i$来说，此时$\\boldsymbol w^T \\boldsymbol x_i \\gt 0,\\hat{y_i}=1,y_i=0$或$\\boldsymbol w^T \\boldsymbol x_i \\lt 0,\\hat{y_i}=0,y_i=1$,综合考虑两种情形可得： $$(\\hat{y_i}-y_i)\\boldsymbol w \\boldsymbol x_i>0$$ 所以可以推得损失函数为： $$L(\\boldsymbol w)=\\sum_{\\boldsymbol x_i \\in M} (\\hat{y_i}-y_i)\\boldsymbol w \\boldsymbol x_i$$ 损失函数的梯度为： $$\\nabla_w L(\\boldsymbol w)=\\sum_{\\boldsymbol x_i \\in M} (\\hat{y_i}-y_i)\\boldsymbol x_i$$ 随机选取一个误分类点$(\\boldsymbol x_i,y_i)$，对$\\boldsymbol w$进行更新： $$\\boldsymbol w \\leftarrow \\boldsymbol w-\\eta(\\hat{y_i}-y_i)\\boldsymbol x_i=\\boldsymbol w+\\eta(y_i-\\hat{y_i})\\boldsymbol x_i$$ 显然式5.2为$\\boldsymbol w$的第$i$个分量$w_i$的变化情况\n",
    "\n",
    "$$\\Delta \\theta_j = -\\eta g_j$$ [推导]：因为 $$\\Delta \\theta_j = -\\eta \\cfrac{\\partial E_k}{\\partial \\theta_j}$$ 又 $$ \\begin{aligned} \\cfrac{\\partial E_k}{\\partial \\theta_j} &= \\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot\\cfrac{\\partial \\hat{y}_j^k}{\\partial \\theta_j} \\ &= (\\hat{y}_j^k-y_j^k) \\cdot f’(\\beta_j-\\theta_j) \\cdot (-1) \\ &= -(\\hat{y}_j^k-y_j^k)f’(\\beta_j-\\theta_j) \\ &= g_j \\end{aligned} $$ 所以 $$\\Delta \\theta_j = -\\eta \\cfrac{\\partial E_k}{\\partial \\theta_j}=-\\eta g_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 支持向量机\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
