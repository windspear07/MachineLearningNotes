
# 1. k近邻算法

1. 分类：对于新样本，根据其k个最近的训练样本的类别，通过多数表决方式预测。
2. 回归：根据k个最近邻的样本标签值的均值为预测值。
3. 可见这是一个直接预测的过程，是lazy learning的著名代表：

    它实际上利用训练数据集对特征向量空间进行划分，并且作为其分类的"模型"。

    这类学习技术在训练阶段仅仅将样本保存起来，训练时间开销为零，等到收到测试样本后再进行处理。

    那些在训练阶段就对样本进行学习处理的方法称作“急切学习”(eager learning)。
4. 没有参数可学，k是一个超参数。
5. 三要素： k选择、距离度量方式、决策规则

## 1.1. k选择

1. 当$k=1$时的k近邻算法称为最近邻算法，此时将训练集中与$\vec x$最近的点的类别作为$\vec x$的分类。
2. $k$值的选择会对$k$近邻法的结果产生重大影响。

    - 若$k$值较小，则相当于用较小的邻域中的训练样本进行预测，"学习"的偏差减小。只有与输入样本较近的训练样本才会对预测起作用，预测结果会对近邻的样本点非常敏感。若近邻的训练样本点刚好是噪声，则预测会出错。即：$k$值的减小意味着模型整体变复杂，易发生过拟合。
        - 优点：减少"学习"的偏差。
        - 缺点：增大"学习"的方差（即波动较大）。
    - 若$k$值较大，则相当于用较大的邻域中的训练样本进行预测。这时输入样本较远的训练样本也会对预测起作用，使预测偏离预期的结果。即：$k$值增大意味着模型整体变简单。
        - 优点：减少"学习"的方差（即波动较小）。
        - 缺点：增大"学习"的偏差。
3. 应用中，$k$值一般取一个较小的数值。通常采用交叉验证法来选取最优的$k$值。

## 1.2. 距离

闵可夫斯基距离

$$L_p(\vec x_i, \vec x_j )=(\sum_{l=1}^n |x_{i,l}-x_{j,l}|^p)^{1/p},p \ge 1$$

## 1.3. 决策

1. 分类决策，多数表决，也可以加权投票
2. 回归，也可加权计算。

# 2. 特点

优点：精度高、对异常值不敏感、无数据输入假定

缺点：计算复杂度高，需要$N\times N$距离矩阵，复杂度$O(N^2)$，数据样本很大时计算量不可接受；但是数据样本少，泛化能力又差；无法判断特征的重要性；空间复杂度高

使用范围：数值型和标称型

# 3. 流程

1. 收集数据
2. 准备数据，结构化
3. 分析数据
4. 训练算法，但是k近邻不需要训练！
5. 测试
6. 使用算法

# 4. kd树

$k$近邻检索的实现若进行线性扫描，则非常耗时，一般采用kd树提高检索效率。

$kd$树是一种对$k$维空间中的样本点进行存储以便对其进行快速检索的树型数据结构。它是二叉树，表示对$k$维空间的一个划分。

构造$kd$树的过程相当于不断的用垂直于坐标轴的超平面将$k$维空间切分的过程。$kd$树的每个结点对应于一个$k$维超矩形区域。

## 4.1. kd树构建

平衡$kd$树构建算法：

- 输入：$k$维空间样本集$\mathbb{D}=\{\vec x_1,\vec x_2,\dots,\vec x_N\} , \vec x_i \in \chi \subseteq \mathbb{R}^n$
- 输出：$kd$树
- 算法步骤：
    - 构造根结点。根结点对应于包含$\mathbb{D}$的$k$维超矩形。
    选择$x_1$为轴，以$\mathbb{D}$中所有样本的$x_1$坐标的中位数$x_1^*$为切分点，将根结点的超矩形切分为两个子区域，切分产生深度为$1$的左、右子结点。切分超平面为$x_1=x_1^*$：
        - 左子节点对应$x_1<x_1^*$子区域
        - 右子节点对应$x_1>x_1^*$子区域
        - 在切分超平面的点$x_1=x_1^*$保存在根节点。
    - 对深度为$j$的节点，选择$x_l$为切分的坐标轴继续切分，$l=j(mod k) + 1$.本次切分之后，树深度$l+1$.
    - 直到所有节点的两个子域没有样本位置。




时间复杂度：O(tKmn)，其中，t为迭代次数，K为簇的数目，m为记录数，n为维数
空间复杂度：O((m+K)n)，其中，K为簇的数目，m为记录数，n为维数