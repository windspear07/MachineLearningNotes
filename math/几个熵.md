
1. 随机变量的熵

$$H(X)=-\sum p(x)log \ p(x)$$

2. 联合熵，$H(X,Y)$
3. 条件上 $H(Y|X)$

$$H(X,Y)-H(X) \\
=-\sum_{x,y} p(x,y)log \ p(x,y)+\sum_x p(x)log \ p(x) \\
=-\sum_{x,y} p(x,y)log \ p(x,y)+\sum_x (\sum_y p(x,y))log \ p(x) \\
=-\sum_{x,y} p(x,y)log \ p(x,y)+\sum_{x,y} p(x,y)log \ p(x) \\
=-\sum p(x,y)log \frac{p(x,y)}{p(x)} \\
=-\sum p(x,y)log \ p(y|x)$$

4. 相互熵

$$D(p||q)=\sum_x p(x) log \frac{p(x)}{q(x)}=E_{p(x)}log \frac{p(x)}{q(x)}$$

5. 最大熵

无偏原则

最大熵模型的本质，它要解决的问题就是已知X，计算Y的概率，且尽可能让Y的概率最大

实践经验和理论计算都告诉我们，在完全无约束状态下，均匀分布等价于熵最大（有约束的情况下，不一定是概率相等的均匀分布。 比如，给定均值和方差，熵最大的分布就变成了正态分布 ）。

$$max H(Y|X) = \sum_{x,y} p(x,y)log\frac{1}{p(y|x)}$$